---
title: Hallucinating
summary: Everything — everything — that comes out of these “AI” platforms is a “hallucination.” Quite simply, these services are slot machines for content.
bookmarkExternal: https://ethanmarcotte.com/wrote/hallucinating/
bookmarkLabel: ethanmarcotte.com
bookmarkAuthor: Ethan Marcotte
date: 2025-05-20T12:30:27.631Z
updated: ""
location:
  locality: Crawley
  country_name: UK
category:
  - ai
  - llm
  - web
  - tech
  - automation
eleventyExcludeFromCollections: false
---

For those of us that may have sipped a little too much of the "Ai" cool-aid, I like how Ethan challenges the semantics of how we're using the word "hallucination" &mdash; and brings a much needed nudge back to reality.

> If you read about the current crop of “artificial intelligence” tools, you’ll eventually come across the word “hallucinate.” It’s used as a shorthand for any instance where the software just, like, makes stuff up: An error, a mistake, a factual misstep — a lie.
>

And...

> Everything — **everything** — that comes out of these “AI” platforms is a “hallucination.” Quite simply, these services are slot machines for content. They’re playing probabilities: when you ask a large language model a question, it returns answers aligned with the trends and patterns they’ve analyzed in their training data. These platforms **do not know** when they get things wrong; they certainly **do not know** when they get things right. Assuming an “artificial intelligence” platform knows the difference between true and false is like assuming a pigeon can play basketball. It just ain’t built for it.
>
> I’m far from the first to make this point. But it seems to me that when we use a term put forward by the people subsidizing and selling these so-called tools — people who would very much like us to believe that these machines can distinguish true from false — we’re participating in a different kind of hallucination.
>
> And a far worse one, at that.
